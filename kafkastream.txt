import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010.KafkaUtils
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
object qlsvoject {
  def main(args: Array[String]): Unit = {
    var kafkaParams = Map[String, Object](
      "bootstrap.servers" -> "hpca04r01n02.hpc.ford.com:9092,hpca04r02n02.hpc.ford.com:9092,hpca04r03n02.hpc.ford.com:9092,hpcb04r03n02.hpc.ford.com:9092",
      "key.deserializer" -> classOf[StringDeserializer],
      "value.deserializer" -> classOf[StringDeserializer],
      "group.id" -> "dops",
      "auto.offset.reset" -> "latest",
      "enable.auto.commit" -> (false: java.lang.Boolean)
    )
    kafkaParams = kafkaParams + ("security.protocol" -> "SASL_PLAINTEXT")
    import org.apache.spark._
    import org.apache.spark.streaming._
    val conf = new SparkConf().setAppName("streams").setMaster("yarn")
    val streamingContext = new StreamingContext(conf, Seconds(200))
    //    val rootLogger = Logger.getRootLogger()
    //    rootLogger.setLevel(Level.WARN)
    val topics = Array("dsc-10147-qlsvo-col-lz")
    //change vikas_nifi to a topic you created.
    val stream = KafkaUtils.createDirectStream[String, String](
      streamingContext, PreferConsistent, Subscribe[String, String](topics, kafkaParams)
)
streamingContext.checkpoint("hdfs:///user/jamin4/")
//change hdfs path to yours.
//stream.map(record=>(record.value().toString)).print()
//val values= stream.map(record=>(record.value() )
//values.foreachrdd


stream.foreachRDD(rddRaw => {
  val rdd = rddRaw.map(_.value)
  val df = spark.read.json(rdd)
})

streamingContext.start()
streamingContext.awaitTermination()
}

}